
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage{natbib}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{float}
\usepackage{threeparttable}
\usepackage{subfig}
\usepackage{graphicx}
\graphicspath{ {Minecraft/images/} }

% User defined commands

%
\begin{document}
%
%\frontmatter          % for the preliminaries
%
%\pagestyle{headings}  % switches on printing of running heads

%
%\mainmatter              % start of the contributions
%
\title{Transfer Learning for Deep Reinforcement Learning Agents}
%
\titlerunning{Transfer Learning for DRL}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Trevor Barron \and Matthew Whitehead \and Alan Yeung}
%
\authorrunning{Barron et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Trevor Barron, Matthew Whitehead, and Alan Yeung}
%
\institute{Colorado College, Colorado Springs CO 80903, USA\\
\email{\{trevor.barron, matthew.whitehead, alan.yeung\}@coloradocollege.edu}\\ 
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Researchers are using deep neural networks and reinforcement learning to train agents to perform complex tasks at unprecidented levels, even better than humans in many cases. 
However, there are still tasks that are too computationally intense for researchers to practically pursue with the current methods. 
Thus, we explore a method to augment deep reinforcement learning that has the potential to speed up learning time and improve overall performance, transfer learning. 
Specifically, we use a human guided transfer learning, where the source tasks are programmed by a human. 
%We train two agents, one augmented with transfer learning and one not augmented with transfer learning, in a Minecraft like environment and analyze the learning of both agents. 
We find that transfer learning provides a substantial learning boost to the agent in the task we have chosen, and provide evidence that the potential benefits of transfer learning, particularly human guided transfer learning, may be worth the costs. 
\keywords{deep learning, reinforcement learning, transfer learning, convolutional neural networks}
\end{abstract}
%

\section{Introduction}

% TODO cite minecraft world!

Deep reinforcement learning (DRL) is currently being used by researchers to create agents that are out performing any that have come before in artificial intelligence (AI) and machine learning (ML) tasks.
For instance, in \citep{mnih2015human} and \citep{silver2016mastering} researchers were able to utilize DRL to create agents that outperformed human experts in several Atari 2600 games and the extremely complex game of Go respectively.
These are unprecedented achievements and have been important milestones in the advancement of artificial intelligence and machine learning.
However, even with the use of DRL, there are still tasks that are too computationally intense or time consuming for most researchers to practically pursue. 

Thus, we explore the use of transfer learning (TL) as an augmentation to DRL in an attempt to speed up learning. 
Other research has provided evidence that reinforcement learning (RL) and TL used together can significantly speed up an agent's learning. 
Many works focus on developing and analyzing the effects of new TL algorithms on RL agents \citep{taylor2007cross,taylor2008autonomous,ramon2007transfer}. 
Some works, such as \citep{taylor2007transfer}, focus on developing techniques to transform the action value function so that it can be directly applied to new target tasks.
The authors develop a TL method and run analysis similar to ours, comparing the improvement of agents that use TL (TL agents) and agents that do not use TL (non-TL agents).
However, few papers have analyzed the learning speed up of DRL combined with TL, which may provide even better speedup in learning compared to RL and TL. 

In this paper we focus on analyzing both the performance and learning time benefits of using DRL in conjunction with TL. 
We train an agent to perform a simple task in a Minecraft like environment, and show that the combined use of DRL and TL provide a substantial learning speedup.
The DRL network from \citep{mnih2015human} was used in conjunction with human programmed environments for the TL.
Although there are considerable drawbacks and limitations to using human selected source tasks for TL, we believe that these findings show that the potential benefits of DRL and TL combined are significant enough as to outweigh these limitations in many AI and ML tasks. 

% Discuss contribution, http://www.eecs.wsu.edu/~taylorm/Publications/JMLR07-taylor.pdf look at Q value reuse

%
%\cite{Simonyan2014}

%
\section{Background}

We use the same deep convolutional neural network (DCNN) and reinforcement learning (RL) algorithm outlined in \citep{mnih2015human} for our experiments. The network structure is as follows: the input to the DCNN consists of an 84 X 84 X 4 image, the first hidden layer convolves 16 8 X 8 filters with stride 4 with the input image and applies a rectifier non-linearity, the second hidden layer convolves 32 4 X 4 filters with stride 2 and applies another rectifier non-linearity, the final hidden layer consists of 256 fully connected rectifier units, and finally the output layer is a fully connected linear layer with an output for each valid action. As in \citep{mnih2015human} we train our network using only pixel input and the rewards recieved from the actions performed. 

The RL algorithm is a variation of Q-learning where the goal is to generate a function that approximates the policy $Q^{*}(s,a)$, the optimal action-value function, which is defined as the maximum achievable return from following any strategy after seeing sequence $s$ and performing some action $a$, $Q^{*}(s,a)=max_{\pi}{\mathbf{E}}_{s' \sim \epsilon} [r + \gamma max_{a'} Q^{*}(s',a') | s,a]$.
That is, $Q^{*}(s,a)$ is a function that maximizes return ($R_{t}$), that is defined as the reward from the current action ($r$) plus the stream of future rewards till termination ($T$) discounted by $\gamma$ at each time step $t$, $R_{t}=\sum_{t' = t}^{T}\gamma^{t'-t}r_{t'}$.
In order to approximate $Q^{*}(s,a)$, a DCNN is used as our function approximator with weights set to $\theta$ in iteration $i$, $Q(s,a,\theta_{i})$.

In order to have the DCNN approximate $Q^{*}(s,a)$ the DCNN is trained with the goal of minimizing a sequence of loss functions, $L_{i}(\theta_{i}) = {\mathbf{E}}_{s,a \sim p(\cdot)}[(y_{i}-Q(s,a,\theta_{i}))^{2}]$ that change at each iteration $i$, where $y_{i}={\mathbf{E}}_{s' \sim \epsilon} [r + \gamma max_{a'} Q(s',a';\theta_{i-1}) | s,a]$ is the target for iteration $i$ and $p(s,a)$ is a probability distribution over sequence $s$ and actions $a$. 
By updating weights according to the goal of minimizing $L_{i}(\theta_{i})$, $Q(s,a,\theta_{i})$ can be shown to approach $Q^{*}(s,a)$ given sufficient training time. 
For more details on the DCNN structure or the RL algorithm see \citep{mnih2015human}.

The term transfer learning (TL), also called inductive transfer, refers to the transfer of knowledge learned in different, but related contexts \citep{ramon2007transfer}.
Examples of transfer learning abound in real life, such as learning to ride a bike after learning to ride a tricycle or learning to run after learning to walk. 

In a machine learning context, researchers studying TL are concerned with the added benefit that learning one task has on learning another (usually related) task.
Often times TL is used to speed up an agents learning on a desired task, known as the target task, by first training the agent on a source task, before transferring the agent to continue training on the target task. 
The hope is that knowledge from the source task will be transferred to the target task and the learning time necessary for the agent to achieve a threshold performance will be decreased. 

There are various measures for the benefits of TL \citep{taylor2009transfer}, but for our purposes the total time measure is used.
That is, the benefit of TL is measured by the difference in total training time, including training on source tasks, for some threshold performance measure. 

In this paper we use a human programmed version of TL, where the source tasks are preselected by the human, and a make use of a DCNN as our function approximator for the RL algorithm.
The agents (one using TL and one not) perform a simple task described in section~\ref{subsec:setup} and the results from the experiments are described in section~\ref{sec:results}.

%Mention environment and cite

\section{Experiments}\label{sec:experiments}

% Questions to ponder: How are the weights being corrected in the DNN? Does it make sense to give -1 for going backwards (sometimes going backwards is a good idea, i.e. dangerous walkway)? 

In order to analyze the potential benefits of TL, we trained agents augmented with TL and compared their learning to agents not augmented with TL. 
If TL provides learning benefits to agents, then we would expect to see a quicker rate of learning for agents augmented with TL.
That is, agents trained with TL should achieve the same reward on a specific task in less time than agents trained without TL. 
If there is no drastic difference in learning times, or a negligible difference, then there are two possibilities: TL may not be beneficial for the target task or the source tasks selected may not stimulate TL. 

We tested our environment with both the Torch and Caffe deep learning frameworks \citep{collobert2011torch7,jia2014caffe}. For all of our experiments and results we used the Caffe framework as it was more efficient for our purposes compared to Torch. 

\subsection{Setup}\label{subsec:setup}

Our experiments use a simple goal oriented task where the goal is to maximize the reward earned. 
The target task environment is depicted in figure \ref{fig:setup1}.
The agent earns one point of reward for going further in the $Z$ (forward) direction than ever previously (let's call this $Z\_max$), zero points for being at a $Z$ position where $Z=Z\_max$, and negative one point if in a position $Z < Z\_max$.
The negative rewards were meant to increase the efficiency of the agent by decreasing inefficient actions by the agent, such as moving backwards, forwards, backwards, etc. 
The reward of the target task is maximized by creating a bridge of six blocks in size and walking forward to the end of the walkway (figure ~\ref{fig:setup2}). 

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth, natwidth=750,natheight=560]{gap6.png}
    \caption{Target task}
    \label{fig:setup1}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth, natwidth=750,natheight=560]{completed.png}
    \caption{Created bridge}
    \label{fig:setup2}
  \end{minipage}
\end{figure}

\noindent A target task with a gap of six blocks was chosen because that is the maximum number of blocks that can be removed that still allows a bridge to be created. 

To augment agents with TL we created five source tasks to train the agent for the target task.

\begin{figure}
\begin{center}
\begin{tabular}{ccc}
\subfloat{\includegraphics[width=.33\linewidth]{gap1.png}} & \subfloat{\includegraphics[width=.33\linewidth]{gap2.png}} & \subfloat{\includegraphics[width=.33\linewidth]{gap3.png}} \\
\subfloat{\includegraphics[width=.33\linewidth]{gap4.png}} & \subfloat{\includegraphics[width=.33\linewidth]{gap5.png}} & \subfloat{\includegraphics[width=.33\linewidth]{gap6.png}}\\
\end{tabular}
\end{center}
\caption{Progression of TL source tasks from the simplest (walkway with gap size 1) to the target task (walkway with gap size 6).}
\end{figure}
 
The source tasks feature a gap ranging from size one block to five blocks in the pathway. 
The agent starts on the simplest source task, the environment with a gap of one block, and progresses to the most complex source task, the environment with a gap of five blocks, before advancing to the target task, the environment with a gap of six blocks.
By training the agent on simpler source tasks than the target task, the hope is that the agent will learn the optimal policy, creating a bridge, faster than the agents that begin training directly on the target task. 

The TL agent is transitioned to the next most complex source task based on performance.
If the agents average reward from the last ten games is above a certain threshold, then the agent transitions to the next source task, or the target task, if it was on the most complex source task.
The threshold used is the minimum amount of reward necessary for the agent to have created a bridge. 
Specifically, the maximum reward attainable without a bridge is roughly twelve points (3 blocks * 4 points per block), plus a few extra possible points if the agent falls forwords off the edge, thus we use an average threshold score of sixteen over the last ten games.
If the agent recieves an average score of at least sixteen, the agent must have created a bridge successfully at least one time. 
Generally, a ten game average score of at least sixteen indicates that the agent can create bridges somewhat consistently because without a bridge an untrained agent usually garners negative points, with inefficient movements described above. 
% TODO: What about do nothing?
The agents have seven possible actions, move forewords, move backwards, rotate up, rotate down, create a grass block, destroy a grass block, or do nothing. 
These actions were selected because they are the minimum actions needed to create a bridge and achieve a maximum reward, any other actions would be learned away and increase the total training time for both the TL and non-TL agents. 
There are two possible ways for an episode to end, if the agent steps off the bridge, either by walking forwards or backwards off an edge, or if the maximum frame limit per round (500 frames) is reached, the episode ends and a new episode begins with the agent in the starting position. 

As discussed in \citep{mnih2015human}, the agent sees and selects actions every fourth frame, which roughly increases the number of games the agent can play by four times, because it takes substantially less computation to run the episode forward one frame than to have the agent select a new action. 
On every skipped frame the agent's action is repeated, with the exception of break a grass block and create a grass block, which are only performed once every four frames in order to prevent the agent from creating or breaking four blocks inadvertently. 
Each episode lasts for a maximum of 500 frames and each epoch is 5000 frames. 
However, this does not necessarily translate perfectly into ten episodes per epoch because (as often happens) the agent can die by walking off the path.

\section{Results and Discussion}\label{sec:results}

Our results provide evidence that shows TL to be an effective tool in speeding up learning, particularly with a task that includes a delayed reward.
Figure \ref{fig:res1} and figure \ref{fig:res2} show the average game reward over the number of training epochs for two trials. 
The approximate maximum reward achievable (roughly eighty-four points) is represented by the black line.
We use the term ``approximate'' because it is possible for an agent to achieve slightly more than eighty-four points by falling forward off the edge at the end of the walkway.
The red line represents whether or not the agent is able to consistently create a bridge in the target task. 
As discussed in section \ref{subsec:setup}, the approximate minimum number of points achievable after creating a bridge is sixteen points.

% TODO: Add image of end of level

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{test1_results.png}
  \end{center}
  \caption{TL agent Vs. non-TL agent trial one}
  \label{fig:res1}

  \begin{center}
    \includegraphics[width=\textwidth]{test2_results.png}
  \end{center}
  \caption{TL agent Vs. non-TL agent trial two with red dots indicating a change in source task}
  \label{fig:res2}
\end{figure}

The red points in figure \ref{fig:res2} represent the approximate epoch (since task transitions usually occur between epochs) where the agent's source task changed.  
The first red point represents the transition from the source task of gap size one to the source task of gap size two, similarly for the second, third, and fourth red dot, while the fifth red dot represents the transition from the the source task of gap size five to the target task of gap size six. 

% TODO: change "score" on graph to "average reward per game", clip epochs so that they match

Figure \ref{fig:res1} and figure \ref{fig:res2} depict a clear learning advantage later on for the TL agent compared to the non-TL agent. 
Both graphs show the TL agent under-performing the non-TL agent in terms of reward early on.
This difference in rewards early on may be attributable to the difference in tasks for the TL agent compared to the non-TL agent, particularly it may have been challenging for the TL agent to generalize the lessons learned from source tasks to new tasks.

However, once the TL agent transitions to the target task, the average game reward (which is an indicator of learning) steadily increases.
Specifically, when looking at figure \ref{fig:res2} after training on all source tasks, the TL agent vastly out performs the non-TL agent in terms of average reward achieved.
It is interesting to observe that as the average game reward of the TL agent increases at a steady rate (after reaching the target task), the average game reward of the non-TL agent appears to level off for a significant time (roughly one hundred epochs).
The average reward of the non-TL agent levels off around the maximum achievable reward without creating a bridge (about twelve points), which indicates that the non-TL agent was having a difficult time discovering the optimal policy, creating a bridge. 
In order for the non-TL agent to have any notion of the benefits of creating a bridge, the non-TL agent must first choose (often randomly) to place six blocks to cover the gap, and then walk across.
That is, the reward is delayed until several ``correct'' actions are performed. 
As the graphs indicate, this was quite a time consuming challenge for the non-TL agent. 
On the other hand, the TL agent was able to grasp the vital concept of creating a bridge early on as it trained on each source task. 
Once the TL agent transitioned to the target task it simply needed to apply its previous learning to create a bridge and learn efficient movements to accumulate reward. 

Table \ref{tab:1} displays the number of epochs needed for each agent to consistently create a bridge and achieve the maximum achievable reward respectively (where the former must be done before the latter). 
Since the training epochs for non-TL and TL agents take the exact same time, the number of epochs also represent the total amount of time an agent took to learn the goal. 

\renewcommand{\arraystretch}{1.2}
\begin{table}[htb]
  \begin{threeparttable}
    \caption{Epochs taken to achieve goal}\label{tab:1}
    \begin{tabular}{l|c|c|c|c} 
      \hline
      & \multicolumn{2}{|c|}{Trail 1} & \multicolumn{2}{|c}{Trail 2} \\
      \hline
                     & Built Bridge & \multicolumn{1}{p{2.5cm}|}{\centering Max Achievable Reward} & Built Bridge & \multicolumn{1}{p{2.5cm}}{\centering Max Achievable Reward} \\
      \hline
      TL agent       & 186          & 261                      & 190          & 381                      \\
      Non-TL agent   & 311          & N/A                      & 307          & N/A                      \\
      Percent change & -40\%        & N/A                      & -38\%        & N/A                      \\ 
      \hline
    \end{tabular}
    \begin{tablenotes}[flushleft]\footnotesize
    \item{Note: The table shows the number of epochs (time) taken by each agent to build a bridge and achieve the maximum achievable score in the target task, where percent change$=($TL epochs$-$non-TL epochs$)/$non-TL epochs.}
    \end{tablenotes}
  \end{threeparttable}
\end{table}

Table \ref{tab:1} shows that the TL agent was able to consistently build a bridge in thirty-nine percent less time, on average, compared to the non-TL agent. 
Additionally, the table indicates that the TL agent was able to earn the maximum achievable reward consistently while the non-TL agent was unable to earn the maximum achievable reward during the allotted training time. 
It is important to note that the time taken by the TL agent to earn the maximum achievable reward is not consistent (261 epochs in trial one vs. 381 epochs in trial two), however given that the non-TL agent was never able to achieve a similar feat, the inconsistency may not be a large concern. 
These results indicate, quite clearly, the advantages in total training time when using TL methods. 

These results do indicate that TL combined with RL has the potential to substantially speed up learning. 
However, it is important to acknowledge the limitations of our use of TL, the first and foremost being that source tasks were selected by humans. 
To date, there is no program that uses fully automated TL, that is no program can successfully select an appropriate source task from a target task, learn how the source and target tasks are related, and effectively transfer knowledge from the source task to the target task, and our methods are no exception \citep{taylor2009transfer}. 
The performance benefits of TL undeniably depend heavily on the source tasks selected, and since our TL methods use human selected/programmed source tasks our results may vary drastically with different source and target tasks. 
Another limitation of TL in general is that there is no set method to choose source tasks that guarantees TL will occur.
In order for TL to occur, the source and target tasks must be similar enough that an agent can generalize learning, but different enough that the source task helps in the learning of the target task.
Currently, there is no measure for the similarity of source and target tasks that guarantees the effectiveness of the source task in TL \citep{taylor2009transfer}.
Additionally, the performance benefits of TL may vary from trial to trial, as the difference in the number of epochs taken to acheive the maixmum achievable reward by the TL agent (see table \ref{tab:1}) indicate. 
Thus, as with many TL methods, there are no guarantees as to how much better a TL agent will perform compared to a non-TL agent, as the algorithms we use have an element of randomness. 
%Even though the performance benefits of TL are not as consistent as one would like, it is still important to remember that the TL agent vastly outperformed the non-TL agent that was unable to earn the maximum achievable reward in the training time. 

Even with these limitations, our results indicate that TL may provide substantial benefits to RL that are worth the costs.
An average learning speed up of 39\% (or even 10\%) is quite substantial when it takes days to train an agent. 
Thus, even if there is the possibility that TL may provide learning speed up, it is worthwhile for researchers to consider human guided TL as a method to increase learning and potential performance. 


%Idea is similar to leveling up in video games, you become better at some task with each level up, but you might not have mastered any task at a specific level.

\section{Future Work}

Fortunately, the limitations of our TL methods provide promising avenues for future research. 
There is some work being done on automating TL source selection, but full automation may still be a while away. 
However, creating a semi automated TL algorithm seems like a particularly promising research avenue. 
%Some researchers, such as 
It may be possible to train an agent to act like a coach and select source tasks, from a set (or sets) of human provided tasks, to train an agent effectively. 
Depending on how the sets of tasks are defined, the ``coach'' may even be somewhat generalizble and may be able to train agents on other tasks with a different set of provided source tasks. 

Additionally, it may be possible to train a DNN to approximate the similarity of a source task and target task and aid in TL training of agents. 
If a DNN could approximate the similarity of a source task and target task to an accurate (for some definition of acurate) enough degree, then we would be one step closer to fully automating TL.
Finally, more work needs to be done in measuring the benefit of TL on different types of tasks and different selections of source tasks.
Does TL work better with delayed reward tasks or immediate reward tasks?
Would a different combination of source tasks than the ones we chose, such as skipping gaps of even length, lead to faster or slower learning?
More work needs to be done to answer these and many other important questions in the domain of TL combined with DRL. 


%
% ---- Bibliography ----
%
\newpage{}

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}


